# üçé FruitSearch - Fruchterkennung mit Machine Learning

## Einleitung
Diese App wurde w√§hrend eines Mini-Hackathons programmiert, in dem man 3 Aufgaben hatte. Diese war die zweite Aufgabe. Man hatte f√ºr jede Aufgabe 1 1/2 Tage Zeit.

### Aufgabe
- Verwende die OpenFoodFacts API.

## Dev Story
Ich wollte mich schon immer mal mit Core ML & Create ML auseinandersetzen und dachte, das w√§re die perfekte M√∂glichkeit, es produktiv und konzentiert auszuprobieren.

Die Idee war, eine App zu programmieren, die mithilfe von Machine Learning Essen erkennen kann. Anschlie√üend soll sie von einer API entsprechende Daten √ºber dieses Essen laden und anzeigen.
Entsprechend dem Zeitlimit habe ich mich auf bestimmte Fr√ºche beschr√§nkt, n√§mlich Orange, Banane, Apfel (und Tomate).

Als erstes habe ich die API mit einfachen URL Requests kennengelernt. Anschlie√üend habe ich mich direkt mit Core ML besch√§ftigt und mir WWDC Sessions auf der Apple Developer App, Youtube Tutorials und die Dokumentation angeschaut. Es war sehr spannend zu sehen, was damit alles m√∂glich ist. Da ich ein Model brauchte, das Objekte auf Fotos erkennen kann, machte ich mich auf die Suche nach Trainigsdaten. Ich w√§hlte Adobe Stock als Quelle. Pro Frucht habe ich mir als erstes nur 10 Fotos ausgesucht und damit ein Model mit Create ML trainiert. 
Als Test, programmierte ich zuerst die Funktion, ein Foto aus der Foto Library auszuw√§hlen, um anschlie√üend das Foto dem Model als Input zu geben. Es war ziemlich cool, die ersten Ergebnisse zu sehen. 
Jedoch wollte ich die Kamera integrieren, was f√ºr mich die schwierigste Aufgabe dieses Projekts war. Daf√ºr verwendete AVFoundation, was ich davor ebenfalls noch nie verwendet hatte. 
Was die Daten der Frucht angeht, wollte ich zuerst nur die Daten der Frucht allein laden und anzeigen. Da jedoch die API nicht n√ºtzliche Daten unverarbeitetr Lebensmittel hergibt, habe ich mich entschieden, einen URLRequest zu verwenden, der mit dem Namen der Frucht nach Produkten sucht.
Nachdem alles funktionierte, programmierte ich noch schicke Animationen mit SwiftUI, die die UX einen Feinschlief geben sollte. 
Am Ende habe ich mich entschieden ein neues Model zu trainieren, mit 15 Fotos pro Frucht.

Das Ergebnis war gut und ausreichend. Dieses Projekt hat mir eine bessere Aussicht gegeben, was man alles programmieren kann, und mich f√ºrs Programmieren noch mehr motiviert. Es war ein sehr cooles Projekt und es ist sehr erstaunlich, was man alles schaffen kann, wenn man bei einem Mini Hackathon mitmacht, bei dem man um einiges produktiver und konzentrierter arbeitet.

(Die OpenFoodFacts API wurde mit der Spoonacular API getauscht!)


